{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7923639,"sourceType":"datasetVersion","datasetId":4656640},{"sourceId":7928621,"sourceType":"datasetVersion","datasetId":4659954}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pymorphy2\nfrom nltk.corpus import stopwords\nimport re\nfrom collections import Counter\nimport math\nimport pandas as pd\n\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nfrom datasets import Dataset\nimport evaluate\n\nstop_words = stopwords.words('russian')\nlemmatizer = pymorphy2.MorphAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:17.248605Z","iopub.execute_input":"2024-03-24T09:29:17.248989Z","iopub.status.idle":"2024-03-24T09:29:21.065366Z","shell.execute_reply.started":"2024-03-24T09:29:17.248944Z","shell.execute_reply":"2024-03-24T09:29:21.064512Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/poemsfortraining/allmergedpoemsfinal.csv')\n\ndf = df[(df['emotionality'] != 500) & (df['emotionality'] != 2)]\ndf.drop(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:21.066785Z","iopub.execute_input":"2024-03-24T09:29:21.067092Z","iopub.status.idle":"2024-03-24T09:29:21.210307Z","shell.execute_reply.started":"2024-03-24T09:29:21.067066Z","shell.execute_reply":"2024-03-24T09:29:21.209513Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def lemmatize_text(text):\n    text = text.lower()\n    text = re.sub('[^а-яА-Я]', ' ', text)\n    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    split_text = text.split(' ')\n    split_text = [lemmatizer.parse(word)[0].normal_form for word in split_text if not word in stop_words and len(word) >2]\n    return \" \".join(split_text)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:14:36.975415Z","iopub.execute_input":"2024-03-24T09:14:36.975687Z","iopub.status.idle":"2024-03-24T09:14:36.982470Z","shell.execute_reply.started":"2024-03-24T09:14:36.975663Z","shell.execute_reply":"2024-03-24T09:14:36.981561Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#df['lem_text'] = df['text'].apply(lemmatize_text)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:14:36.985346Z","iopub.execute_input":"2024-03-24T09:14:36.985687Z","iopub.status.idle":"2024-03-24T09:14:36.991779Z","shell.execute_reply.started":"2024-03-24T09:14:36.985655Z","shell.execute_reply":"2024-03-24T09:14:36.990900Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"data_dict = {\n    'text': df['text'].tolist(),\n    'meaningfulness': df['meaningfulness'].tolist(),\n    'grammar': df['grammar'].tolist(),\n    'emotionality': df['emotionality'].tolist(),\n}\n\ndef combine_labels(row):\n    return [float(row['meaningfulness']), float(row['grammar']), float(row['emotionality'])]\n\nlabels = df.apply(combine_labels, axis=1).tolist()\n\ndata_dict['labels'] = labels\n\ndataset = Dataset.from_dict(data_dict)\n\ndataset = dataset.train_test_split(test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:23.528613Z","iopub.execute_input":"2024-03-24T09:29:23.529415Z","iopub.status.idle":"2024-03-24T09:29:24.404159Z","shell.execute_reply.started":"2024-03-24T09:29:23.529385Z","shell.execute_reply":"2024-03-24T09:29:24.403298Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_path = 'DeepPavlov/rubert-base-cased'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:25.730583Z","iopub.execute_input":"2024-03-24T09:29:25.731616Z","iopub.status.idle":"2024-03-24T09:29:26.760525Z","shell.execute_reply.started":"2024-03-24T09:29:25.731574Z","shell.execute_reply":"2024-03-24T09:29:26.759412Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c69f370e67458093ee52306a04028f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cdb7c95a48c47af89f0ee034779be39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24a4fefb3354f6aa855f342a789035c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cb91b32a604572a4369a2427c20626"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function(dataset):\n    return tokenizer(dataset['text'],padding=True, truncation=True,max_length=50, add_special_tokens = True)\n\ndataset = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:28.084124Z","iopub.execute_input":"2024-03-24T09:29:28.084747Z","iopub.status.idle":"2024-03-24T09:29:28.775900Z","shell.execute_reply.started":"2024-03-24T09:29:28.084713Z","shell.execute_reply":"2024-03-24T09:29:28.774754Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"797d3e09697e45de90b88aeee9047139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68ebc7860da4f34ac64bb88bc9cb239"}},"metadata":{}}]},{"cell_type":"code","source":"class2id = {'meaningfulness': 0,\n 'grammar': 1,\n 'emotionality': 2}\nid2class = {0: 'meaningfulness',\n 1: 'grammar',\n 2: 'emotionality'}\nclasses = ['meaningfulness', 'grammar', 'emotionality']","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:31.190761Z","iopub.execute_input":"2024-03-24T09:29:31.191658Z","iopub.status.idle":"2024-03-24T09:29:31.196230Z","shell.execute_reply.started":"2024-03-24T09:29:31.191625Z","shell.execute_reply":"2024-03-24T09:29:31.195210Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:33.526999Z","iopub.execute_input":"2024-03-24T09:29:33.527596Z","iopub.status.idle":"2024-03-24T09:29:44.885494Z","shell.execute_reply.started":"2024-03-24T09:29:33.527565Z","shell.execute_reply":"2024-03-24T09:29:44.884423Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-03-24 09:29:35.730789: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-24 09:29:35.730924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-24 09:29:35.895784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n\ndef sigmoid(x):\n    return 1/(1 + np.exp(-x))\n\ndef compute_metrics(eval_pred):\n    \n    predictions, labels = eval_pred\n    predictions = sigmoid(predictions)\n    predictions = (predictions > 0.6).astype(int).reshape(-1)\n  \n    labels_np = np.array(labels)  \n    return clf_metrics.compute(predictions=predictions, references=labels_np.astype(int).reshape(-1))","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:44.887398Z","iopub.execute_input":"2024-03-24T09:29:44.888020Z","iopub.status.idle":"2024-03-24T09:29:49.695540Z","shell.execute_reply.started":"2024-03-24T09:29:44.887984Z","shell.execute_reply":"2024-03-24T09:29:49.694760Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367dd704ab874cb98a8c7585075ed143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76646658820d4648a8a6f9d9f6bc5410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d84a84a92d446d88a5ea4610b995a55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d84fa069fba4093a29461076b578571"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3, id2label=id2class, label2id=class2id, problem_type = \"multi_label_classification\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:49.696614Z","iopub.execute_input":"2024-03-24T09:29:49.696890Z","iopub.status.idle":"2024-03-24T09:29:53.782952Z","shell.execute_reply.started":"2024-03-24T09:29:49.696866Z","shell.execute_reply":"2024-03-24T09:29:53.782209Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02c410db0cd482ca589fe246b4b91a1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n\n   output_dir=\"rubert-final\",\n   learning_rate=2e-5,\n   per_device_train_batch_size=9,\n   per_device_eval_batch_size=9,\n   num_train_epochs=2,\n   weight_decay=0.01,\n   evaluation_strategy=\"epoch\",\n   save_strategy=\"epoch\",\n   load_best_model_at_end=True,\n   push_to_hub=False,\n   hub_token = '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',\n)\n\ntrainer = Trainer(\n\n   model=model,\n   args=training_args,\n   train_dataset=dataset[\"train\"],\n   eval_dataset=dataset[\"test\"],\n   tokenizer=tokenizer,\n   data_collator=data_collator,\n   compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:29:53.784549Z","iopub.execute_input":"2024-03-24T09:29:53.784852Z","iopub.status.idle":"2024-03-24T09:31:29.335673Z","shell.execute_reply.started":"2024-03-24T09:29:53.784827Z","shell.execute_reply":"2024-03-24T09:31:29.334515Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240324_093003-nw4rj3g7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/karpovapolina666/huggingface/runs/nw4rj3g7' target=\"_blank\">winter-dream-8</a></strong> to <a href='https://wandb.ai/karpovapolina666/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/karpovapolina666/huggingface' target=\"_blank\">https://wandb.ai/karpovapolina666/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/karpovapolina666/huggingface/runs/nw4rj3g7' target=\"_blank\">https://wandb.ai/karpovapolina666/huggingface/runs/nw4rj3g7</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [132/132 00:50, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.597243</td>\n      <td>0.673956</td>\n      <td>0.781721</td>\n      <td>0.708199</td>\n      <td>0.872277</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.592261</td>\n      <td>0.675944</td>\n      <td>0.765692</td>\n      <td>0.741876</td>\n      <td>0.791089</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=132, training_loss=0.5850453810258345, metrics={'train_runtime': 93.9709, 'train_samples_per_second': 24.923, 'train_steps_per_second': 1.405, 'total_flos': 60176916437400.0, 'train_loss': 0.5850453810258345, 'epoch': 2.0})"},"metadata":{}}]}]}