{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"В этой тетрадке сначала тестируем accuracy и f1 модели отдельно по классам (meaningfulness, grammar, emotionality), затем считаем эти параметры для сгенерированных поэм. ","metadata":{}},{"cell_type":"markdown","source":"# Тестируем accuracy и f1 по классам","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForSequenceClassification\n\nmodel_name_or_path = 'numblilbug/rubert-cased-poem-evalutation' \n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_test_list = [[int(elem) for elem in sublist] for sublist in dataset['test']['labels']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_classes = []\nfor i in dataset['test']['text']:\n    inputs = tokenizer(i, return_tensors=\"pt\",padding=True, truncation=True,max_length=50, add_special_tokens = True)\n    outputs = model(**inputs)\n    predicted_logits = outputs.logits\n    predicted_classes = (predicted_logits > 0.6).int()\n    test_classes.append(predicted_classes.tolist()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef calculate_scores_per_class(true_list, pred_list):\n    num_classes = len(true_list[0])\n    accuracy_per_class = []\n    f1_scores_per_class = []\n\n    for i in range(num_classes):\n        true_class = [item[i] for item in true_list]\n        pred_class = [item[i] for item in pred_list]\n\n        accuracy = accuracy_score(true_class, pred_class)\n        f1_score_class = f1_score(true_class, pred_class)\n\n        accuracy_per_class.append(accuracy)\n        f1_scores_per_class.append(f1_score_class)\n\n    return accuracy_per_class, f1_scores_per_class\n\n\n\naccuracy_per_class, f1_scores_per_class = calculate_scores_per_class(true_test_list, test_classes)\nprint(\"Accuracy per class:\", accuracy_per_class)\nprint(\"F1 Scores per class:\", f1_scores_per_class)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Определяем критерии для сгенерированных поэм для дальнейшего сравнения моделей","metadata":{}},{"cell_type":"code","source":"df_gen = pd.read_csv('/kaggle/input/gen-poems/poems_gen - 1.csv')\ndf_gen = df_gen[0:100]\ndf_gen = df_gen.sort_values('model')\ndf_gen = df_gen.reset_index()\ndf_gen = df_gen.drop('index', axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_text = df_gen['text'].to_list()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_classes = []\nfor i in gen_text:\n    inputs = tokenizer(i, return_tensors=\"pt\",padding=True, truncation=True,max_length=50, add_special_tokens = True)\n    outputs = model(**inputs)\n    predicted_logits = outputs.logits\n    predicted_classes = (predicted_logits > 0.6).int()\n    gen_classes.append(predicted_classes.tolist()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transposed_list = list(map(list, zip(*gen_classes)))\n\ndf_gen['meaningfulness'], df_gen['grammar'], df_gen['emotionality'] = transposed_list\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_gen.to_csv('gen_poems.csv')","metadata":{},"execution_count":null,"outputs":[]}]}